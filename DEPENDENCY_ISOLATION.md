# LibAMM PyTorch Dependency Isolation

## ğŸ¯ ç›®æ ‡

**å°† PyTorch ä¾èµ–å®Œå…¨éš”ç¦»åœ¨ LibAMM å†…éƒ¨ï¼Œè®© SAGE å¯ä»¥ä½¿ç”¨ LibAMM è€Œä¸éœ€è¦å®‰è£… PyTorch**

## ğŸ“Š é—®é¢˜èƒŒæ™¯

### åŸå§‹æ¶æ„
```
SAGE (Python)
  â””â”€> import PyAMM
      â””â”€> requires torch in Python environment âŒ
          â””â”€> LibAMM.so (uses PyTorch internally)
```

**é—®é¢˜**ï¼š
- SAGE ç”¨æˆ·å¿…é¡»å®‰è£… PyTorchï¼ˆå¤§å‹ä¾èµ– ~2GBï¼‰
- PyTorch ç‰ˆæœ¬å†²çªï¼ˆCUDA vs CPUï¼‰
- å¢åŠ äº† SAGE çš„å®‰è£…å¤æ‚åº¦

### æ–°æ¶æ„ï¼ˆä¾èµ–éš”ç¦»ï¼‰
```
SAGE (Python)
  â””â”€> import PyAMM
      â””â”€> NumPy interface (no torch dependency) âœ…
          â””â”€> LibAMM.so (PyTorch isolated inside .so file)
```

**ä¼˜åŠ¿**ï¼š
- âœ… SAGE ç”¨æˆ·åªéœ€è¦ NumPyï¼ˆè½»é‡çº§ï¼‰
- âœ… PyTorch ç¼–è¯‘è¿› LibAMM.soï¼Œä¸å½±å“ Python ç¯å¢ƒ
- âœ… ç®€åŒ– SAGE å®‰è£…æµç¨‹

## ğŸ”§ æŠ€æœ¯å®ç°

### æ ¸å¿ƒæ”¹åŠ¨

**1. ç§»é™¤ Python å±‚çš„ PyTorch ä¾èµ–**

`src/PyAMM.cpp` ä¿®æ”¹ï¼š
```cpp
// ä¹‹å‰
#include <torch/extension.h>  // éœ€è¦ Python ç¯å¢ƒæœ‰ torch

// ä¹‹å
#include <pybind11/numpy.h>   // åªéœ€è¦ NumPy
#include <torch/torch.h>        // ä»…å†…éƒ¨ä½¿ç”¨ï¼Œä¸æš´éœ²åˆ° Python
```

**2. åˆ›å»ºç±»å‹è½¬æ¢å±‚**

```cpp
// torch::Tensor â†’ numpy.ndarray
py::array torch_to_numpy(const torch::Tensor& tensor);

// numpy.ndarray â†’ torch::Tensor  
torch::Tensor numpy_to_torch(py::array array);
```

**3. åŒ…è£…ç±»**

```cpp
class CPPAlgoWrapper {
    AbstractCPPAlgoPtr algo_;  // å†…éƒ¨ä½¿ç”¨ PyTorch
public:
    // NumPy æ¥å£
    py::array amm(py::array A, py::array B, uint64_t sketchSize) {
        torch::Tensor torchA = numpy_to_torch(A);
        torch::Tensor torchB = numpy_to_torch(B);
        torch::Tensor result = algo_->amm(torchA, torchB, sketchSize);
        return torch_to_numpy(result);  // è¿”å› NumPy
    }
};
```

### Python ä½¿ç”¨ç¤ºä¾‹

```python
# ç”¨æˆ·ä»£ç  - åªéœ€è¦ NumPyï¼Œä¸éœ€è¦ torch
import numpy as np
import PyAMM  # ä¸å†éœ€è¦ import torch

# åˆ›å»º NumPy æ•°ç»„
A = np.random.randn(1000, 500).astype(np.float32)
B = np.random.randn(500, 800).astype(np.float32)

# åˆ›å»ºç®—æ³•ï¼ˆå†…éƒ¨ä½¿ç”¨ PyTorchï¼Œä½†å¯¹ç”¨æˆ·é€æ˜ï¼‰
algo = PyAMM.createAMM("crs")
cfg = PyAMM.ConfigMap()
cfg.edit("sketchRatio", 0.1)
algo.setConfig(cfg)

# è®¡ç®—ï¼ˆè¾“å…¥è¾“å‡ºéƒ½æ˜¯ NumPyï¼‰
C = algo.amm(A, B, sketchSize=50)  
# C æ˜¯ numpy.ndarrayï¼Œä¸æ˜¯ torch.Tensor âœ…
```

## ğŸ“¦ ç¼–è¯‘é…ç½®

### LibAMM ç¼–è¯‘ï¼ˆéœ€è¦ PyTorchï¼‰

```bash
# LibAMM ç¼–è¯‘æ—¶é“¾æ¥ PyTorchï¼ˆé™æ€é“¾æ¥æˆ–åŠ¨æ€é“¾æ¥ï¼‰
cd libamm/build
cmake -DENABLE_PYBIND=ON -DENABLE_TORCHSCRIPT=ON ..
make -j8

# ç”Ÿæˆ PyAMM.soï¼ˆåŒ…å« PyTorch åº“ï¼‰
# æ–‡ä»¶å¤§å°ï¼š~50MBï¼ˆåŒ…å« PyTorch æ ¸å¿ƒï¼‰
```

### SAGE å®‰è£…ï¼ˆä¸éœ€è¦ PyTorchï¼‰

```bash
# SAGE ç”¨æˆ·å®‰è£…
pip install sage-libs  # åªéœ€è¦ NumPyï¼Œä¸éœ€è¦ PyTorch âœ…

# Python ç¯å¢ƒä¾èµ–
# - numpy
# - pybind11
# âœ… NO torch required!
```

## ğŸ§ª æµ‹è¯•çŠ¶æ€

### âœ… å·²å®Œæˆ
- [x] ä»£ç é‡æ„ï¼ˆNumPy æ¥å£ï¼‰
- [x] ç±»å‹è½¬æ¢å±‚å®ç°
- [x] åŒ…è£…ç±»åˆ›å»º
- [x] Git æäº¤ï¼ˆcommit 217b531ï¼‰

### â³ å¾…æµ‹è¯•
- [ ] ç¼–è¯‘ LibAMM.soï¼ˆéœ€è¦è§£å†³ PyTorch CPU/CUDA ç‰ˆæœ¬å†²çªï¼‰
- [ ] NumPy â†” torch::Tensor è½¬æ¢æ­£ç¡®æ€§
- [ ] æ€§èƒ½æµ‹è¯•ï¼ˆè½¬æ¢å¼€é”€ï¼‰
- [ ] SAGE é›†æˆæµ‹è¯•

### âš ï¸ å·²çŸ¥é—®é¢˜

**ç¼–è¯‘ç¯å¢ƒé—®é¢˜**ï¼š
```
å½“å‰ sage ç¯å¢ƒä¸­çš„ PyTorch 2.7.1 éœ€è¦ CUDA
ä½† WSL ç¯å¢ƒæ²¡æœ‰ CUDA
éœ€è¦ä½¿ç”¨ CPU ç‰ˆæœ¬çš„ PyTorch ç¼–è¯‘ LibAMM
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **é€‰é¡¹ A**ï¼šåœ¨æœ‰ CUDA çš„æœºå™¨ä¸Šç¼–è¯‘ LibAMM.so
2. **é€‰é¡¹ B**ï¼šåˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ libamm-build ç¯å¢ƒï¼ˆå®‰è£… PyTorch CPU ç‰ˆï¼‰
3. **é€‰é¡¹ C**ï¼šä½¿ç”¨ Docker å®¹å™¨ç¼–è¯‘

## ğŸ“ˆ åŠŸèƒ½ä¿ç•™æƒ…å†µ

### âœ… å®Œæ•´ä¿ç•™ï¼ˆ90%+ï¼‰

**CPPAlgosï¼ˆç®—æ³•ï¼‰**ï¼š
- âœ… CRS, CRSV2, BCRSï¼ˆColumn Row Sampling ç³»åˆ—ï¼‰
- âœ… Weighted-CRï¼ˆåŠ æƒé‡‡æ ·ï¼‰
- âœ… CountSketch, EWS, CoOFD, TugOfWarï¼ˆSketch ç®—æ³•ï¼‰
- âœ… SMP-PCA, BlockLRA, RIP, FastJLTï¼ˆé™ç»´ç®—æ³•ï¼‰
- âœ… INT8, PQ-Rawï¼ˆé‡åŒ–ç®—æ³•ï¼‰

**MatrixLoadersï¼ˆæ•°æ®åŠ è½½ï¼‰**ï¼š
- âœ… Random, Gaussian, Beta, Binomialï¼ˆéšæœºçŸ©é˜µï¼‰
- âœ… Sparse, MNIST, SIFTï¼ˆæ•°æ®é›†åŠ è½½å™¨ï¼‰
- âœ… Mtxï¼ˆMatrix Market æ ¼å¼ï¼‰

### âš ï¸ éœ€è¦æ•°æ®è½¬æ¢ï¼ˆ3 ä¸ªç±»ï¼‰

**ä¾èµ– torch::jitï¼ˆéœ€è¦ .pt æ–‡ä»¶ï¼‰**ï¼š
- VectorQuantization - éœ€è¦ codebooks.pt
- ProductQuantizationHash - éœ€è¦ hash_containers.pt
- MediaMillMatrixLoader - éœ€è¦ MediaMill.pt

**è§£å†³æ–¹æ¡ˆ**ï¼šæä¾›æ•°æ®è½¬æ¢å·¥å…·ï¼ˆ.pt â†’ .npyï¼‰

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³è¡ŒåŠ¨ï¼ˆç¼–è¯‘æµ‹è¯•ï¼‰
```bash
# 1. åˆ›å»ºç‹¬ç«‹çš„ç¼–è¯‘ç¯å¢ƒ
conda create -n libamm-build python=3.11
conda activate libamm-build
pip install torch --index-url https://download.pytorch.org/whl/cpu
pip install pybind11 numpy

# 2. ç¼–è¯‘ LibAMM
cd libamm/build
cmake -DENABLE_PYBIND=ON ..
make -j8

# 3. æµ‹è¯• NumPy æ¥å£
python -c "
import numpy as np
import PyAMM
A = np.random.randn(100, 50).astype(np.float32)
B = np.random.randn(50, 80).astype(np.float32)
algo = PyAMM.createAMM('crs')
C = algo.amm(A, B, 30)
print('Success! C shape:', C.shape, 'dtype:', C.dtype)
"
```

### åç»­ä¼˜åŒ–
1. **æ€§èƒ½ä¼˜åŒ–**ï¼šå‡å°‘ NumPy â†” Tensor è½¬æ¢å¼€é”€
2. **å†…å­˜ä¼˜åŒ–**ï¼šä½¿ç”¨ zero-copy è½¬æ¢ï¼ˆå…±äº«å†…å­˜ï¼‰
3. **æ•°æ®è½¬æ¢å·¥å…·**ï¼šä¸º VQ/PQ ç®—æ³•æä¾› .pt â†’ .npy è½¬æ¢è„šæœ¬
4. **æ–‡æ¡£**ï¼šç¼–å†™ SAGE ç”¨æˆ·æŒ‡å—

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒä»·å€¼
- âœ… **ä¾èµ–éš”ç¦»**ï¼šPyTorch ä¸å†æ±¡æŸ“ Python ç¯å¢ƒ
- âœ… **å‘åå…¼å®¹**ï¼šAPI ä¸å˜ï¼Œåªæ˜¯ç±»å‹ä» torch.Tensor å˜ä¸º numpy.ndarray
- âœ… **åŠŸèƒ½å®Œæ•´**ï¼š90%+ çš„ç®—æ³•æ— éœ€ä¿®æ”¹å³å¯ä½¿ç”¨
- âœ… **ç®€åŒ–å®‰è£…**ï¼šSAGE ç”¨æˆ·ä¸éœ€è¦å¤„ç† PyTorch ç‰ˆæœ¬å†²çª

### æŠ€æœ¯äº®ç‚¹
- ğŸ¯ å·§å¦™çš„æŠ½è±¡å±‚ï¼šç”¨æˆ·çœ‹åˆ° NumPyï¼Œå†…éƒ¨ä»ç”¨ PyTorch
- ğŸ”§ é›¶æ”¹åŠ¨ç®—æ³•ï¼šæ‰€æœ‰ LibAMM ç®—æ³•ä»£ç ä¿æŒä¸å˜
- ğŸ“¦ å¯åˆ†å‘ï¼šPyAMM.so å¯ä»¥ä½œä¸ºç‹¬ç«‹çš„äºŒè¿›åˆ¶åˆ†å‘

---

**Commit**: `217b531` - feat: Isolate PyTorch dependency in Python bindings using NumPy interface  
**Branch**: `main-dev`  
**Date**: 2025-11-12
